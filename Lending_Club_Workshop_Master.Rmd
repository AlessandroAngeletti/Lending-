---
title: "Data Science I, Workshop I: Predicting interest rates at the Lending Club"
author: "Change this to your GROUP NUMBER"
date: "5/10/2020"
output:
  html_document:
    theme: cerulean
    highlight: zenburn
    number_sections: yes
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, load_libraries, include = FALSE}
library(tidyverse) # the usual stuff: dplyr, readr, and other goodies
library(lubridate) # to handle dates
library(GGally) # for correlation-scatterplot matrix
library(car) # vif() function to check for multicolinearity
library(ggfortify) # to produce residual diagnostic plots
library(rsample) # to split dataframe in training- & testing sets
library(here) # to read files and organise data
library(janitor) # clean_names()
library(broom) # use broom:augment() to get tidy table with regression output, residuals, etc
library(huxtable) # to get summary table of all models produced
library(caret) # to train more advanced models (k-fold cross-validation, stepwise regression, LASSO)
library(zoo) #to allow for timeseries operations

```


This workshop partially replicates the analysis presented in class (lectures 1 and 2) and builds on it. You are to work in your study group. Feel free to refer to the "Lending_Club_Session1_and_2.html" if you get stuck.

The workshop consists of 10 questions plus an optional question. Submit one report per group.  *Please write your answers after each question and submit a knitted RMD markup file via canvas within 6 days after the end of this workshop.* Please keep your answers concise -- focus on answering what you are asked and use your data science work to justify your answers. Do not focus on the process you have followed to reach the answer.


# Load and prepare the data

We start by loading the data to R in a dataframe.
```{r, load_data, warning=FALSE, message=FALSE}

lc_raw <- read_csv("LendingClub Data.csv",  skip=1) %>%  #since the first row is a title we want to skip it. 
  clean_names() # use janitor::clean_names()
```

# ICE the data: Inspect, Clean, Explore

Any data science engagement starts with ICE. Inspect, Clean and Explore the data. For this workshop I have cleaned the data for you. 

```{r}
glimpse(lc_raw) 

lc_clean<- lc_raw %>%
  dplyr::select(-x20:-x80) %>% #delete empty columns
  filter(!is.na(int_rate)) %>%   #delete empty rows
  mutate(
    issue_d = mdy(issue_d),  # lubridate::mdy() to fix date format
    term = factor(term_months),     # turn 'term' into a categorical variable
    delinq_2yrs = factor(delinq_2yrs) # turn 'delinq_2yrs' into a categorical variable
  ) %>% 
  dplyr::select(-emp_title,-installment, -term_months, everything()) #move some not-so-important variables to the end. 


glimpse(lc_clean) 
```

The data is now in a clean format stored in the dataframe "lc_clean." 

# Q1. Explore the data by building some visualizations as suggested below. Fill free to add your own. 

Provide your answers in the code block below. (Look at the "Lending_Club_Session1_and_2.html" for some hints on how to do this using ggplot.)

```{r, data_visualisation}
# Build a histogram of interest rates. Make sure it looks nice!
ir_hist <- lc_clean %>% 
  ggplot() +
  geom_histogram(aes(x = int_rate), bins = 20) +
  scale_x_continuous(labels = scales::percent) +
  theme_bw() +
  labs(
    title = "Distribution of interest rates on loan",
    subtitle = "Right skwede distribution of IRs with an aervage of ~ 10%"
      ) +
  theme_bw()

ir_hist

# Build a histogram of interest rates but use different color for loans of different grades 
ir_hist_two <- lc_clean %>% 
  ggplot(aes(x = int_rate, fill = grade)) +
  geom_histogram(binwidth = 0.01) + 
  scale_x_continuous(labels = scales::percent) +
  labs(x = "Interest Rate in %'s",
       title = "Distribution of interest rates by loan grade",
       subtitle = "Higher Credit Risk Comands a Higher Interest Premium"
       ) +
  theme_bw()

ir_hist_two

# Produce a scatter plot of loan amount against interest rate and add visually the line of best fit
ir_scatter <- lc_clean %>% 
  ggplot(aes(y = int_rate, x = loan_amnt)) +
  geom_point(alpha = 0.5) + 
  geom_smooth(method = "lm", se = 0) +
  scale_y_continuous(labels = scales::percent) +
  scale_x_continuous(labels = scales::dollar) +
  labs(y = "Interest Rate in %'s",
       title = "Scatter plot of loan amount against interest rate",
       subtitle = "Higher loan ammounts comand a higher liquidity premium as its hard to obtain requiered funds"
       ) +
  theme_bw()

ir_scatter

# Produce a scatter plot of annual income against interest rate and add visually the line of best fit 
ir_scatter_two <- lc_clean %>% 
  #filter(annual_inc < 2500000) %>% 
  ggplot(aes(y = int_rate, x = annual_inc)) +
  geom_point(alpha = 0.5, size = 0.2) + 
  geom_smooth(method = "lm", se = 0) +
  scale_x_continuous(labels = scales::dollar) +
  labs(y = "Interest Rate in %'s",
       title = "Scatter plot of annual income against interest rate",
       subtitle = "Interest rate correlated to a higher income, due to a high correlation between income and amount"
       ) +
  theme_bw()

ir_scatter_two

# In the same axes, produce box plots of the interest rate for every value of delinquencies
ir_box <- lc_clean %>% 
  group_by(delinq_2yrs) %>% 
  ggplot(aes(x = int_rate, y = delinq_2yrs)) +
  geom_boxplot() +
  scale_x_continuous(labels = scales::percent) +
  labs(x = "Interest Rate in %'s",
       title = "Box plots of the interest rate for every value of delinquencies",
       subtitle = "The higher the delinq the higher the IR as the less realiable of a debtor you are"
       ) +
  theme_bw()
  
ir_box
```

> Variability of IRs decreases with each deliq. This is likely due to a lower sample size.

# Estimate simple linear regression models

We start with a simple but quite powerful model.

```{r, simple regression}
#Use the lm command to estimate a regression model with the following variables "loan_amnt",  "term", "dti", "annual_inc", and "grade"

lc_pair <- lc_clean %>% 
  select(int_rate, loan_amnt, term, annual_inc, grade) %>% 
  ggpairs()

lc_pair

model1<-lm(
  int_rate ~ loan_amnt + term + dti + annual_inc + grade,
  data = lc_clean
  )

summary(model1)

```

## Q2. Answer the following questions on model 1.{-}

a. Are all variables statistically significant?
b. How do you interpret the coefficient of the Term60 dummy variable? Of the grade B dummy variable? 
c. How much explanatory power does the model have? 
d. Approximately, how wide would the 95% confidence interval of any prediction based on this model be? 


> Question a
> No, annual_inc is not statistically significant as the p-value is greater than 0.05. In opinion, we believe that this is logical because the annual income is high correlated with the loan amount.

> Question b
> That holding a 60 month term loan commands an interest rate that is 0.0036% higher than a 30 month term loan all else held constant.
> Holding  a grade B loan (instead of a grade A loan), comands a rate that is 0.00355% higher all else held constant.

> Question c
> By looking at the adjusted R-squared, we find that the model explains 91.97% of the variability in the data. Note that we use the adjusted R-squared instead of the regular R-Squared because we want to penalise our model for holding an excessive number of variables.

> Question d
> The Confidence Interal is found as `+/- 2 * SE`. Therefore, we find that for this model, the width of the CI would 0.04224% [=2*0.02112%].

# Feature Engineering

Let's build progressively more complex models with more features.

```{r, Feature Engineering}
# Add to the previous model an interaction between loan amount and grade. Use the "var1*var2" notation to define an interaction term in the linear regression model. This will add the interaction and the individual variables to the model.

model2 <- lm(int_rate ~ term + dti + annual_inc + loan_amnt*grade,
            data = lc_clean)
summary(model2)

# Add to the model the square and the cube of annual income. Use the poly(var_name,3) command as a variable in the linear regression model.  

model3 <- lm(int_rate ~ term + dti + annual_inc + poly(annual_inc, 2) + poly(annual_inc, 3)+ loan_amnt*grade,
            data = lc_clean) 

# Continuing with the previous model, instead of annual income as a continuous variable break it down into quartiles and use quartile dummy variables. You can do this with the following command. 
  
lc_clean <- lc_clean %>% 
  mutate(quartiles_annual_inc = as.factor(ntile(annual_inc, 4)))

model4 <- lm(int_rate ~ term + dti + quartiles_annual_inc + loan_amnt*grade,
            data = lc_clean)

# Compare the performance of these four models using the anova command
anova(model1, model2, model3, model4)
huxtable::huxreg(model1, model2, model3, model4)
  
```

## Q3. Answer the following questions {-}

a. Which of the four models has the most explanatory power in sample?
b. In model 2, how do you interpret the estimated coefficient of the interaction term between grade B and loan amount? 
c. The problem of multicolinearity describes the situations where one feature is highly correlated with other fueatures (or with a linear combination of other features). If your goal is to use the model to make predictions, should you be concerned by the problem of multicolinearity? Why, or why not?

> Question a
> The explaantory power of all four models is identical.
> We believe that this is the case because each enw itteration of the mode adds variables whose coefficients are either zero or statistically insignficant. Hence, each iteration does not improve the model at all, causing the R-sqaured to remain the same.

> Question b
> For every dollar increase that occurs to loans specifically labels as `Grade B` the interest rate of the loan will increase by 0.000%.

> Question c
> Yes, multicollinearty **is not** a huge issue when we're predicting because dont really *mind why* this occurs as all we want to do is accurately model what would occur in real life. However, if we're infering, we do care because we want to be able to isolate and understand the impact of each individual variable on the results produced. 

# Out of sample testing

Let's check the predictive accuracy of model2 by holding out a subset of the data to use as testing. This method is sometimes refered to as the hold-out method for out-of-sample testing.

```{r, out of sample testing}
#split the data in dataframe called "testing" and another one called  "training". The "training" dataframe should have 80% of the data and the "testing" dataframe 20%.

set.seed(1234)
train_test_split <- initial_split(lc_clean, prop = 0.8)
training <- training(train_test_split)
testing <- testing(train_test_split)

# Fit model2 on the training set 
model2_training<-lm(int_rate ~ loan_amnt + term+ dti + annual_inc + grade +grade:loan_amnt, training)

# Calculate the RMSE of the model in the training set (in sample)

rmse_training<-sqrt(mean((residuals(model2_training))^2))

# Use the model to make predictions out of sample in the testing set

pred<-predict(model2_training,testing)

# Calculate the RMSE of the model in the testing set (out of sample)

rmse_testing<- RMSE(pred,testing$int_rate)

rmse_training
rmse_testing
```

## Q4. How much does the predictive accuracy of model 2 deteriorate when we move from in sample to out of sample testing? Is this sensitive to the random seed chosen? Is there any evidence of overfitting? {-}

> The predictive accuracy (as measured by the rmse metric) becomes marginally worse when we move from in-sample testing to out-of-sample testing.
> It does not seem to be [partocuarlyparticularly sentisitve to the random seed chosen as after 10 different itteration, he difference only seems to be within a +/- 2% rage difference
> No because the rmse is rather low and insentive to changes in rmse accross different seeds.

# k-fold cross validation

We can also do out of sample testing using the method of k-fold cross validation. Using the caret package this is easy.

```{r, k-fold cross validation}
#the method "cv" stands for cross validation. We re going to create 10 folds.  

control <- trainControl (
    method = "cv",
    number = 10,
    verboseIter = TRUE) #by setting this to true the model will report its progress after each estimation

#we are going to train the model and report the results using k-fold cross validation
plsFit <- train(
    int_rate ~ loan_amnt + term + dti + annual_inc + grade + grade:loan_amnt ,
    lc_clean,
   method = "lm",
    trControl = control
   )
  

summary(plsFit)
print(plsFit)
```

## Q5. Compare the out-of-sample RMSE of 10-fold cross validation and the hold-out method. Are they different? Which do you think is more reliable? Are there any drawbacks to the k-fold cross validation method compared to the hold-out method? {-}

> The RMSEs of both method are very similar as they are within ~0.01 of each other.
> We believe the K-Fold cross validation method is far more reliable as it allows us to continously resample of data for as many time as we wish to do this. Moreover by taking the error of each predictor error, we can avoid outliers. Something that cannot be avoided with the hold-out-method.
> The k-fold cross validation method is far more computationally intense as we require out devices to recompute the predictor error k times as opposed to completing this process just once. This can lead to high waiting time for large samples with many folds.


# Sample size estimation and learning curves

We can use the hold out method for out-of-sample testing to check if we have a sufficiently large sample to estimate the model reliably. The idea is to set aside some of the data as a testing set. From the remaining data draw progressively larger training sets and check how the performance of the model on the testing set changes. If the performance no longer improves with larger datasets we know we have a large enough sample.  The code below does this. Examine it and run it with different random seeds. 

```{r, learning curves}
#select a testing dataset (25% of all data)
set.seed(12)

train_test_split <- initial_split(lc_clean, prop = 0.75)
remaining <- training(train_test_split)
testing <- testing(train_test_split)

#We are now going to run 30 models starting from a tiny training set drawn from the training data and progressively increasing its size. The testing set remains the same in all iterations.

#initiating the model by setting some parameters to zero
rmse_sample <- 0
sample_size<-0
Rsq_sample<-0

for(i in 1:30) {
#from the remaining dataset select a smaller subset to training the data
set.seed(100)
sample

  learning_split <- initial_split(remaining, prop = i/200)
  training <- training(learning_split)
  sample_size[i] = nrow(training)
  
  #traing the model on the small dataset
  model3 <- lm(int_rate ~ loan_amnt + term+ dti + annual_inc + grade + grade:loan_amnt, training)
  #test the performance of the model on the large testing dataset. This stays fixed for all iterations.
  pred <- predict(model3,testing)
  rmse_sample[i] <- rmse(pred,testing$int_rate)
  Rsq_sample[i] <- R2(pred,testing$int_rate)
}
plot(sample_size,rmse_sample)
plot(sample_size,Rsq_sample)
```

## Q6. Using the learning curves above, approximately how large of a sample size would we need to estimate model 3 reliably? Once we reach this sample size, if we want to reduce the prediction error further what options do we have?{-}

> When sample size gets to 1000, our predictions start getting very reliable. However, they can marginally improve if we increase sample size all the way to 2000. Therefore, for the model to be considered very relaibly, we would need a sample size of 2000. After this sample size, increasing the size of the sameple brings us no benefits. Therefore, we should instead attempt to either:

*@Nitya all you bud*


# Regularization using LASSO regression

If we are in the region of the learning curve where we do not have enough data, one option is to use a regularization method such as LASSO.

Let's try to estimate large and complicated model (many interactions and polynomials) on a small training dataset using OLS regression and hold-out validation method.

```{r, OLS model overfitting}

#split the data in testing and training. The training test is really small.
set.seed(1234)
train_test_split <- initial_split(lc_clean, prop = 0.01)
training <- training(train_test_split)
testing <- testing(train_test_split)

model_lm<-lm(int_rate ~ poly(loan_amnt,3) + term+ dti + annual_inc + grade +grade:poly(loan_amnt,3):term +poly(loan_amnt,3):term +grade:term, training)
predictions <- predict(model_lm,testing)

# Model prediction performance
data.frame(
  RMSE = rmse(predictions, testing$int_rate),
  Rsquare = R2(predictions, testing$int_rate)
)
```

Not surprisingly this model does not perform well -- as we knew form the learning curves we constructed for a simpler model we need a lot more data to estimate this model reliably. Try running it again with different seeds. The model's performance tends to be sensitive to the choice of the training set.

LASSO regression offers one solution -- it extends the OLS regression by penalizing the model for setting any coefficient to a value that is different from zero. The penalty is proportional to a parameter $\lambda $ (pronounced lambda). This parameter cannot be estimated directly (and for this reason sometimes it is refered to as hyperparameter) and will be selected through k-fold cross validation in order to provide the best out-of-sample performance.  As result of the LASSO precedure, only those features that are more strongly associated with the outcome will have non-zero coefficients and the estimated model will be less sensitive to the training set. Sometimes LASSO regression is refered to as regularization. 

```{r, LASSO compared to OLS, warning=FALSE, message=FALSE}
#we will look for the optimal lambda in this sequence (we will try 1000 different lambdas)
set.seed(6969)
lambda_seq <- seq(0, 0.01, length = 1000)
#lasso regression using k-fold cross validation to select the best lambda

lasso <- train(
 int_rate ~ poly(loan_amnt,3) + term+ dti + annual_inc + grade +grade:poly(loan_amnt,3):term +poly(loan_amnt,3):term +grade:term,
 data = training,
 method = "glmnet",
  preProc = c("center", "scale"), #This option standardizes the data before running the LASSO regression
  trControl = control,
  tuneGrid = expand.grid(alpha = 1, lambda = lambda_seq) #alpha=1 specifies to run a LASSO regression. If alpha=0 the model would run ridge regression.
  )


# Model coefficients
coef(lasso$finalModel, lasso$bestTune$lambda)

#Best lambda
lasso$bestTune$lambda

# Count of how many coefficients are greater than zero and how many are equal to zero
sum(coef(lasso$finalModel, lasso$bestTune$lambda)!=0)
sum(coef(lasso$finalModel, lasso$bestTune$lambda)==0)

# Make predictions
predictions <- predict(lasso,testing)

# Model prediction performance
data.frame(
  RMSE = rmse(predictions, testing$int_rate),
  Rsquare = R2(predictions, testing$int_rate)
)

```

## Q7. Answer the following questions {-}
a. Which model performs best out of sample, OLS regression or LASSO? 
b. What value of lambda offers best performance? Is this sensitive to the random seed?
c. How many coefficients are zero and how many are non-zero in the LASSO model of best fit? 
d. Why is it important to standardize continuous variables before running LASSO?

> The LASSO method works much **much** better as the RMSE is far lower (0.0103 v 0.0446) and the Rsquared is far larger (0.923 v 0.352).
> $\lambda $ = 0.00012 with the seed we used. Yes, the seed dramtically changes the coefficient as seen by running the code multiple times.
> There are exactly 22 coefficient that are non zero and there are exactly 36 coeifficients that are zero.
> Since coefficients of different variables are measured in different units, it is important to standardize any continuous variable (subtract the mean and divide by standard deviation). Otherwise results will be misleading!

# Using Time Information
Let's try to further improve the model's predictive performance. So far we have not used any time series information. Effectively, all things being equal, our prediction for the interest rate of a loan given in 2009 would be the same as that of a loan given in 2011. Is this a good assumption?
 
First, investigate graphically whether there are any time trends in the interest rates. (Note that the variable "issue_d" only has information on the month the loan was awarded but not the exact date.) Can you use this information to further improve the forecasting accuracy of your model? Try controlling for time in a linear fashion (i.e., a linear time trend) and controlling for time as quarter dummies (this is a method to capture non-linear effects of time -- we assume that the impact of time doesn't change within a quarter but it can chance from quarter to quarter). Finally, check if time affect loans of different grades differently.

```{r, time trends}

#linear time trend (add code below)

#linear time trend by grade (add code below)


#Train models using OLS regression and k-fold cross-validation
#The first model has some explanatory variables and a linear time trend

time1<-train(
  int_rate ~ ,#fill your variables here "+ issue_d"
  lc_clean,
  method = "lm",
  trControl = control)

summary(time1)

#The second model has a different linear time trend for each grade class
time2<-train(
    int_rate ~ , #fill your variables here 
    lc_clean,
   method = "lm",
    trControl = control
   )
  

summary(time2)

#Change the time trend to a quarter dummy variables.
#zoo::as.yearqrt() creates quarter dummies 
lc_clean_quarter<-lc_clean %>%
  mutate(yq = as.factor(as.yearqtr(lc_clean$issue_d, format = "%Y-%m-%d")))



time3<-train(
    int_rate ~ ,#fill your variables here 
    lc_clean_quarter,
     method = "lm",
    trControl = control
   )
  
summary(time3)

#We specify one quarter dummy variable for each grade. This is going to be a large model as there are 19 quarters x 7 grades = 133 quarter-grade dummies.
time4<-train(
    int_rate ~  ,#fill your variables here 
    lc_clean_quarter,
     method = "lm",
    trControl = control
   )

summary(time4)

data.frame(
  time1$results$RMSE,
  time2$results$RMSE,
  time3$results$RMSE,
  time4$results$RMSE)


```
## Q8 Based on your analysis above, is there any evidence to suggest that interest rates change over time? Does including time trends /quarter dummies imrpove predictions? {-}

>Answer here:

# Using Bond Yields 
One concern with using time trends for forecasting is that in order to make predictions for future loans we will need to project trends to the future. This is an extrapolation that may not be reasonable, especially if macroeconomic conditions in the future change. Furthermore, if we are using quarter dummies, it is not even possible to estimate the coefficient of these dummy variables for future quarters.

Instead, perhaps it's better to find the reasons as to why different periods are different from one another. The csv file "MonthBondYields.csv" contains information on the yield of US Treasuries on the first day of each month. Can you use it to see if you can improve your predictions without using time dummies? 


```{r, bond yields}
#load the data to memory as a dataframe
bond_prices<-readr::read_csv("MonthBondYields.csv")

#make the date of the bond file comparable to the lending club dataset
#for some regional date/number (locale) settings this may not work. If it does try running the following line of code in the Console
#Sys.setlocale("LC_TIME","English")
bond_prices <- bond_prices %>%
  mutate(Date2=as.Date(paste("01",Date,sep="-"),"%d-%b-%y")) %>%
  select(-starts_with("X"))

#let's see what happened to bond yields over time. Lower bond yields mean the cost of borrowing has gone down.

bond_prices %>%
  ggplot(aes(x=Date2, y=Price))+geom_point(size=0.1, alpha=0.5)

#join the data using a left join
lc_with_bonds<-lc_clean %>%
  left_join(bond_prices, by = c("issue_d" = "Date2")) %>%
  arrange(issue_d) %>%
  filter(!is.na(Price)) #drop any observations where there re no bond prices available

# investigate graphically if there is a relationship 
lc_with_bonds%>%
  ggplot(aes(x=int_rate, y=Price))+geom_point(size=0.1, alpha=0.5)+geom_smooth(method="lm")

lc_with_bonds%>%
  ggplot(aes(x=int_rate, y=Price, color=grade))+geom_point(size=0.1, alpha=0.5)+geom_smooth(method="lm")

#let's train a model using the bond information


plsFit<-train(
    int_rate ~  , #fill your variables here 
    lc_with_bonds,
   method = "lm",
    trControl = control
   )
summary(plsFit)
```
## Q9. Do bond yields have any explanatory power?

>Answer here: 


## Q10. Choose a model and describe your methodology {-}
Feel free to investigate more models with different features using the methodologies covered so far. Present the model you believe predicts interest rates the best. Describe how good it is (including the approximate length of the 95% Confidence Interval of predictions that use this model) and what features it uses. What methodology did you use to choose it? (Do not use time trends or quarter dummies in your model as the first cannot be extrapolated into the future reliably and the second cannot be even estimated for future quarters.)

>Answer here:

## Q11. (optional) Use other publicly available datasets to further improve performance (e.g., quarterly data on US inflation or [CPI](https://fred.stlouisfed.org/series/CPALTT01USQ657N)). Explain why you think the additional data will make a difference and check if it does.{-}
